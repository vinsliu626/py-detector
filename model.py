import torch
import re
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from collections import OrderedDict
import math


class GPT2PPL:
    """
    Perplexity + Burstiness based AI detector (GPTZero-ish style)
    - No training required
    - Works best for English prose with >= ~120 words
    """

    def __init__(self, model_id="gpt2-medium"):
        self.model_id = model_id

        # ✅ device for deploy (Render CPU)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model = GPT2LMHeadModel.from_pretrained(model_id).to(self.device)
        self.model.eval()
        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

        self.max_length = self.model.config.n_positions
        self.stride = 512

        # ====== Filtering (prevents titles/short fragments from polluting stats) ======
        self.min_words_per_line = 6          # ignore very short lines (titles/fragments)
        self.min_chars_per_line = 30
        self.invalid_ppl_cutoff = 1e8        # treat >= cutoff as invalid (e.g., 1e9 fallback)

        # ====== Sentence bucket thresholds (kept from your style) ======
        self.ai_ppl_th = 80
        self.likely_ai_ppl_th = 130

        # ====== Overall score mapping (tweak later if needed) ======
        # avg_ppl: lower => more AI
        self.overall_ppl_center = 80.0       # around this ppl, AI-ness starts dropping
        self.overall_ppl_width = 40.0        # how quickly ppl affects score

        # burstiness: lower => more AI (AI tends to be smoother / less variable)
        self.overall_burst_center = 0.55     # around this burst, AI-ness starts dropping
        self.overall_burst_width = 0.35

        # weights
        self.w_ppl = 0.75
        self.w_burst = 0.10
        self.w_bucket = 0.15

    # ----------------------- helpers -----------------------
    @staticmethod
    def _clamp(x, a=0.0, b=1.0):
        return max(a, min(b, x))

    def getResults(self, threshold):
        """
        threshold is avg perplexity per valid sentence.
        lower => more AI-like
        """
        if threshold < 60:
            label = 0
            return "The Text is generated by AI.", label
        elif threshold < 80:
            label = 0
            return "The Text most probably contains parts generated by AI. (require more text for better judgement)", label
        else:
            label = 1
            return "The Text is written by Human.", label

    def _split_lines(self, sentence: str):
        """
        More stable splitting:
        - split by blank lines/newlines into chunks
        - then split each chunk into sentences by punctuation
        """
        sentence = sentence.replace("\r", "")
        chunks = re.split(r"\n+", sentence)
        lines = []
        for c in chunks:
            c = c.strip()
            if not c:
                continue
            parts = re.split(r"(?<=[.!?])\s+", c)
            for p in parts:
                p = p.strip()
                if p:
                    lines.append(p)
        return lines

    def _is_valid_line(self, line: str) -> bool:
        line = line.strip()
        if not line:
            return False
        if re.search(r"[a-zA-Z0-9]+", line) is None:
            return False
        if len(line) < self.min_chars_per_line:
            return False
        if len(line.split()) < self.min_words_per_line:
            return False
        return True

    def _overall_ai_score(self, avg_ppl: float, burstiness: float, ai_gen_count: int, likely_ai_count: int, denom: int):
        """
        Returns ai01 in [0,1] as overall AI-likelihood score.
        - avg_ppl lower => ai01 higher
        - burstiness lower => ai01 higher
        - bucket ratio slightly boosts/penalizes
        """
        denom = max(1, denom)

        # ppl signal: avg_ppl=40 -> high AI, avg_ppl=80 -> low AI
        ppl_ai = self._clamp((self.overall_ppl_center - avg_ppl) / self.overall_ppl_width)

        # burst signal: burst=0.2 -> high AI, burst=0.55 -> low AI
        burst_ai = self._clamp((self.overall_burst_center - burstiness) / self.overall_burst_width)

        # bucket signal: AI + 0.5*likely normalized by denom
        bucket_ai = self._clamp((ai_gen_count + 0.5 * likely_ai_count) / denom)

        ai01 = self._clamp(self.w_ppl * ppl_ai + self.w_burst * burst_ai + self.w_bucket * bucket_ai)
        return ai01

    # ----------------------- main -----------------------
    def __call__(self, sentence):
        results = OrderedDict()

        # quick guard
        total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
        total_valid_char = sum(len(x) for x in total_valid_char)
        if total_valid_char < 100:
            return {"status": "Please input more text (min 100 characters)"}, "Please input more text (min 100 characters)"

        lines = self._split_lines(sentence)

        # doc perplexity (informational)
        doc_ppl = self.getPPL(sentence)
        results["Perplexity"] = doc_ppl

        ppl_list = []
        valid_lines = []
        ai_gen = []
        likely_ai = []
        human = []

        for line in lines:
            if not self._is_valid_line(line):
                continue

            ppl = self.getPPL(line)

            # ignore invalid
            if (not (ppl == ppl)) or ppl == float("inf") or ppl >= self.invalid_ppl_cutoff:
                continue

            valid_lines.append(line)
            ppl_list.append(ppl)

            if ppl < self.ai_ppl_th:
                ai_gen.append(line)
            elif ppl < self.likely_ai_ppl_th:
                likely_ai.append(line)
            else:
                human.append(line)

        if len(ppl_list) < 3:
            return {"status": "Not enough valid sentences after filtering. Please input more text."}, "Not enough valid sentences after filtering. Please input more text."

        avg_ppl = sum(ppl_list) / len(ppl_list)
        results["Perplexity per line"] = avg_ppl

        # burstiness = std/mean
        mean_ppl = avg_ppl
        var = sum((x - mean_ppl) ** 2 for x in ppl_list) / len(ppl_list)
        sd = math.sqrt(var)
        burstiness = sd / mean_ppl if mean_ppl > 0 else 0.0
        results["Burstiness"] = burstiness

        out, label = self.getResults(avg_ppl)
        results["label"] = label

        denom = max(1, len(valid_lines))

        # sentence bucket percentages (still useful for debug/UI)
        ai_gen_percentage = round(len(ai_gen) / denom * 100, 2)
        likely_ai_percentage = round(len(likely_ai) / denom * 100, 2)
        human_percentage = round(len(human) / denom * 100, 2)

        results["AI Generated percentage"] = ai_gen_percentage
        results["Most probably AI Generated percentage"] = likely_ai_percentage
        results["Human percentage"] = human_percentage
        results["valid_sentences"] = denom

        # ✅ Overall AI score (GPTZero-ish)
        ai01 = self._overall_ai_score(
            avg_ppl=avg_ppl,
            burstiness=burstiness,
            ai_gen_count=len(ai_gen),
            likely_ai_count=len(likely_ai),
            denom=denom,
        )
        results["AI overall"] = round(ai01 * 100, 2)

        return results, out

    def getPPL(self, sentence):
        encodings = self.tokenizer(sentence, return_tensors="pt")
        input_ids_all = encodings.input_ids.to(self.device)
        seq_len = input_ids_all.size(1)

        # too short -> invalid
        if seq_len < 2:
            return 1e9

        nlls = []
        prev_end_loc = 0
        end_loc = 0

        for begin_loc in range(0, seq_len, self.stride):
            end_loc = min(begin_loc + self.max_length, seq_len)
            trg_len = end_loc - prev_end_loc

            input_ids = input_ids_all[:, begin_loc:end_loc]
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids, labels=target_ids)
                neg_log_likelihood = outputs.loss * trg_len

            nlls.append(neg_log_likelihood)

            prev_end_loc = end_loc
            if end_loc == seq_len:
                break

        val = torch.exp(torch.stack(nlls).sum() / max(1, end_loc))
        ppl = float(val.item())

        if not (ppl == ppl) or ppl == float("inf"):
            return 1e9

        return ppl
