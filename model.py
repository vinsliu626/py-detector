import math
import re
from collections import OrderedDict

import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast


class GPT2PPL:
    """
    Perplexity + Burstiness based AI detector (GPTZero-ish style)
    - Algorithm unchanged (PPL/burst/bucket/overall mapping).
    - Engineering: CPU-only, single-load model, lower overhead.
    """

    def __init__(self, model_id="gpt2-medium"):
        self.model_id = model_id

        # CPU only: reduce overhead on shared infra
        torch.set_grad_enabled(False)
        torch.set_num_threads(1)

        # Load ONCE (your old code loaded twice -> huge RAM)
        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
        self.model = GPT2LMHeadModel.from_pretrained(
            model_id,
            low_cpu_mem_usage=True,
        )
        self.model.eval()

        self.max_length = self.model.config.n_positions
        self.stride = 512

        # Filtering
        self.min_words_per_line = 6
        self.min_chars_per_line = 30
        self.invalid_ppl_cutoff = 1e8

        # Thresholds
        self.ai_ppl_th = 80
        self.likely_ai_ppl_th = 130

        # Overall mapping
        self.overall_ppl_center = 80.0
        self.overall_ppl_width = 40.0

        self.overall_burst_center = 0.55
        self.overall_burst_width = 0.35

        # Weights
        self.w_ppl = 0.75
        self.w_burst = 0.10
        self.w_bucket = 0.15

    @staticmethod
    def _clamp(x, a=0.0, b=1.0):
        return max(a, min(b, x))

    def getResults(self, threshold):
        if threshold < 60:
            label = 0
            return "The Text is generated by AI.", label
        elif threshold < 80:
            label = 0
            return "The Text most probably contains parts generated by AI. (require more text for better judgement)", label
        else:
            label = 1
            return "The Text is written by Human.", label

    def _split_lines(self, sentence: str):
        sentence = sentence.replace("\r", "")
        chunks = re.split(r"\n+", sentence)
        lines = []
        for c in chunks:
            c = c.strip()
            if not c:
                continue
            parts = re.split(r"(?<=[.!?])\s+", c)
            for p in parts:
                p = p.strip()
                if p:
                    lines.append(p)
        return lines

    def _is_valid_line(self, line: str) -> bool:
        line = line.strip()
        if not line:
            return False
        if re.search(r"[a-zA-Z0-9]+", line) is None:
            return False
        if len(line) < self.min_chars_per_line:
            return False
        if len(line.split()) < self.min_words_per_line:
            return False
        return True

    def _overall_ai_score(self, avg_ppl: float, burstiness: float, ai_gen_count: int, likely_ai_count: int, denom: int):
        denom = max(1, denom)

        ppl_ai = self._clamp((self.overall_ppl_center - avg_ppl) / self.overall_ppl_width)
        burst_ai = self._clamp((self.overall_burst_center - burstiness) / self.overall_burst_width)
        bucket_ai = self._clamp((ai_gen_count + 0.5 * likely_ai_count) / denom)

        ai01 = self._clamp(self.w_ppl * ppl_ai + self.w_burst * burst_ai + self.w_bucket * bucket_ai)
        return ai01

    def __call__(self, sentence: str):
        results = OrderedDict()

        total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
        total_valid_char = sum(len(x) for x in total_valid_char)
        if total_valid_char < 100:
            return {"status": "Please input more text (min 100 characters)"}, "Please input more text (min 100 characters)"

        lines = self._split_lines(sentence)

        doc_ppl = self.getPPL(sentence)
        results["Perplexity"] = doc_ppl

        ppl_list = []
        valid_lines = []
        ai_gen = []
        likely_ai = []
        human = []

        for line in lines:
            if not self._is_valid_line(line):
                continue

            ppl = self.getPPL(line)

            if (not (ppl == ppl)) or ppl == float("inf") or ppl >= self.invalid_ppl_cutoff:
                continue

            valid_lines.append(line)
            ppl_list.append(ppl)

            if ppl < self.ai_ppl_th:
                ai_gen.append(line)
            elif ppl < self.likely_ai_ppl_th:
                likely_ai.append(line)
            else:
                human.append(line)

        if len(ppl_list) < 3:
            return {"status": "Not enough valid sentences after filtering. Please input more text."}, "Not enough valid sentences after filtering. Please input more text."

        avg_ppl = sum(ppl_list) / len(ppl_list)
        results["Perplexity per line"] = avg_ppl

        mean_ppl = avg_ppl
        var = sum((x - mean_ppl) ** 2 for x in ppl_list) / len(ppl_list)
        sd = math.sqrt(var)
        burstiness = sd / mean_ppl if mean_ppl > 0 else 0.0
        results["Burstiness"] = burstiness

        out, label = self.getResults(avg_ppl)
        results["label"] = label

        denom = max(1, len(valid_lines))
        results["AI Generated percentage"] = round(len(ai_gen) / denom * 100, 2)
        results["Most probably AI Generated percentage"] = round(len(likely_ai) / denom * 100, 2)
        results["Human percentage"] = round(len(human) / denom * 100, 2)
        results["valid_sentences"] = denom

        ai01 = self._overall_ai_score(avg_ppl, burstiness, len(ai_gen), len(likely_ai), denom)
        results["AI overall"] = round(ai01 * 100, 2)

        return results, out

    def getPPL(self, sentence: str) -> float:
        encodings = self.tokenizer(sentence, return_tensors="pt")
        seq_len = encodings.input_ids.size(1)

        if seq_len < 2:
            return 1e9

        nlls = []
        prev_end_loc = 0

        for begin_loc in range(0, seq_len, self.stride):
            end_loc = min(begin_loc + self.max_length, seq_len)
            trg_len = end_loc - prev_end_loc
            input_ids = encodings.input_ids[:, begin_loc:end_loc]
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids, labels=target_ids)
                neg_log_likelihood = outputs.loss * trg_len

            nlls.append(neg_log_likelihood)
            prev_end_loc = end_loc
            if end_loc == seq_len:
                break

        val = torch.exp(torch.stack(nlls).sum() / max(1, end_loc))
        ppl = float(val.item())

        if not (ppl == ppl) or ppl == float("inf"):
            return 1e9
        return ppl
